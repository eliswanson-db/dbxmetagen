{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b98f003f-aff9-4b6e-949d-63455a83a25d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Any columns that all statistics are computed for will have fresher data and more detail.\n",
    "\n",
    "This is not meant to be a production-capable notebook but an example so the approach is clear.\n",
    "\n",
    "This can take a while for big tables.\n",
    "\n",
    "Used successfully on internal POCs to quickly get all metadata added to tables for comment generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db9d2097-f7b1-4c93-ad21-e8e0294d5323",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from typing import List, Optional\n",
    "import os\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def compute_statistics_for_tables_from_csv(\n",
    "    csv_path: str, \n",
    "    table_column_name: str = \"table_names\",\n",
    "    skip_errors: bool = True,\n",
    "    limit: Optional[int] = None\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Read table names from a CSV file and compute statistics for all columns in each table.\n",
    "    \n",
    "    Args:\n",
    "        csv_path (str): Path to the CSV file containing table names\n",
    "        table_column_name (str): Name of the column in the CSV that contains table names\n",
    "        skip_errors (bool): Whether to continue processing if an error occurs for a table\n",
    "        limit (Optional[int]): Maximum number of tables to process, None for all\n",
    "        \n",
    "    Returns:\n",
    "        List[str]: List of tables that were successfully processed\n",
    "    \"\"\"\n",
    "\n",
    "    spark = SparkSession.builder.getOrCreate() ### Initialize Spark session in case this is not being run in a notebook.\n",
    "    \n",
    "    # if not os.path.exists(csv_path):\n",
    "    #     error_msg = f\"CSV file not found at path: {csv_path}\"\n",
    "    #     logger.error(error_msg)\n",
    "    #     raise FileNotFoundError(error_msg)\n",
    "    \n",
    "    try:\n",
    "        tables_df = spark.read.csv(csv_path, header=True, inferSchema=True)\n",
    "\n",
    "        if table_column_name not in tables_df.columns:\n",
    "            available_columns = \", \".join(tables_df.columns)\n",
    "            error_msg = f\"Column '{table_column_name}' not found in CSV. Available columns: {available_columns}\"\n",
    "            logger.error(error_msg)\n",
    "            raise ValueError(error_msg)\n",
    "            \n",
    "        tables_pd = tables_df.toPandas()\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Failed to read CSV with Spark, falling back to pandas: {str(e)}\")\n",
    "        try:\n",
    "            tables_pd = pd.read_csv(csv_path)\n",
    "            if table_column_name not in tables_pd.columns:\n",
    "                available_columns = \", \".join(tables_pd.columns)\n",
    "                error_msg = f\"Column '{table_column_name}' not found in CSV. Available columns: {available_columns}\"\n",
    "                logger.error(error_msg)\n",
    "                raise ValueError(error_msg)\n",
    "        except Exception as e2:\n",
    "            error_msg = f\"Failed to read CSV file: {str(e2)}\"\n",
    "            logger.error(error_msg)\n",
    "            raise RuntimeError(error_msg)\n",
    "    \n",
    "    table_names = tables_pd[table_column_name].dropna().unique().tolist()\n",
    "    logger.info(f\"Found {len(table_names)} unique table names in CSV\")\n",
    "    \n",
    "    if limit is not None and limit > 0:\n",
    "        table_names = table_names[:limit]\n",
    "        logger.info(f\"Limited processing to {limit} tables\")\n",
    "\n",
    "    successful_tables = []\n",
    "    failed_tables = []\n",
    "    \n",
    "    for i, full_table_name in enumerate(table_names):\n",
    "        try:\n",
    "            logger.info(f\"Processing table {i+1}/{len(table_names)}: {full_table_name}\")\n",
    "            try:\n",
    "                spark.catalog.tableExists(full_table_name)\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Table validation failed for {full_table_name}: {str(e)}\")\n",
    "                if not skip_errors:\n",
    "                    raise\n",
    "                failed_tables.append(full_table_name)\n",
    "                continue\n",
    "            \n",
    "            logger.info(f\"Computing statistics for {full_table_name}\")\n",
    "            spark.sql(f\"ANALYZE TABLE {full_table_name} COMPUTE STATISTICS FOR ALL COLUMNS\")\n",
    "            \n",
    "            successful_tables.append(full_table_name)\n",
    "            logger.info(f\"Successfully computed statistics for {full_table_name}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = f\"Error processing table {full_table_name}: {str(e)}\"\n",
    "            logger.error(error_msg)\n",
    "            failed_tables.append(full_table_name)\n",
    "            if not skip_errors:\n",
    "                raise RuntimeError(error_msg)\n",
    "    \n",
    "    logger.info(f\"Processing complete. Successfully processed {len(successful_tables)} tables.\")\n",
    "    if failed_tables:\n",
    "        logger.warning(f\"Failed to process {len(failed_tables)} tables: {', '.join(failed_tables)}\")\n",
    "    \n",
    "    return successful_tables\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        try:\n",
    "            dbutils.widgets.text(\"csv_path\", \"table_names.csv\", \"Path to CSV file with table names\")\n",
    "            dbutils.widgets.text(\"table_column_name\", \"table_names\", \"Column name containing table names\")\n",
    "            dbutils.widgets.dropdown(\"skip_errors\", \"True\", [\"True\", \"False\"], \"Skip errors and continue processing\")\n",
    "            dbutils.widgets.text(\"limit\", \"10\", \"Maximum number of tables to process (optional)\")\n",
    "            \n",
    "            csv_path = os.path.join(os.getcwd(), dbutils.widgets.get(\"csv_path\"))\n",
    "            table_column_name = dbutils.widgets.get(\"table_column_name\")\n",
    "            skip_errors = dbutils.widgets.get(\"skip_errors\") == \"True\"\n",
    "            limit_str = dbutils.widgets.get(\"limit\")\n",
    "            limit = int(limit_str) if limit_str.strip() else None\n",
    "        except NameError:\n",
    "            logger.info(\"Not running in a notebook environment, using default values\")\n",
    "            csv_path = os.path.join(os.path.dirname(os.getcwd()), dbutils.widgets.get(\"csv_path\"))\n",
    "            table_column_name = \"table_name\"\n",
    "            skip_errors = True\n",
    "            limit = None\n",
    "        \n",
    "        successful_tables = compute_statistics_for_tables_from_csv(\n",
    "            csv_path=csv_path,\n",
    "            table_column_name=table_column_name,\n",
    "            skip_errors=skip_errors,\n",
    "            limit=limit\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            display(spark.createDataFrame([(table,) for table in successful_tables], [\"Processed Tables\"]))\n",
    "        except NameError:\n",
    "            print(f\"Processed tables: {successful_tables}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in main function: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4a07eed-af70-464c-9f94-6f801178095e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "analyze_table_all_columns",
   "widgets": {
    "csv_path": {
     "currentValue": "table_names.csv",
     "nuid": "ebd7e2ff-dba3-405e-8f71-4633e391db47",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "table_names.csv",
      "label": "Path to CSV file with table names",
      "name": "csv_path",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "table_names.csv",
      "label": "Path to CSV file with table names",
      "name": "csv_path",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "full_table_name": {
     "currentValue": "",
     "nuid": "c5fd573d-7fb1-4a61-bb60-4f166328889f",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "decision_intelligence.dim.brand",
      "label": null,
      "name": "full_table_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "decision_intelligence.dim.brand",
      "label": null,
      "name": "full_table_name",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    },
    "limit": {
     "currentValue": "10",
     "nuid": "2fe52c8d-f9ec-4165-9879-1f2b6b346216",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "10",
      "label": "Maximum number of tables to process (optional)",
      "name": "limit",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "10",
      "label": "Maximum number of tables to process (optional)",
      "name": "limit",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "skip_errors": {
     "currentValue": "True",
     "nuid": "248c2d41-ed7a-48d0-a4bf-e9fd7d65dd0b",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "True",
      "label": "Skip errors and continue processing",
      "name": "skip_errors",
      "options": {
       "widgetDisplayType": "Dropdown",
       "choices": [
        "True",
        "False"
       ],
       "fixedDomain": true,
       "multiselect": false
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "dropdown",
      "defaultValue": "True",
      "label": "Skip errors and continue processing",
      "name": "skip_errors",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": null,
       "choices": [
        "True",
        "False"
       ]
      }
     }
    },
    "table_column_name": {
     "currentValue": "table_name",
     "nuid": "cc8eb691-b94b-4b7a-b72a-65c47988c39f",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "table_names",
      "label": "Column name containing table names",
      "name": "table_column_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "table_names",
      "label": "Column name containing table names",
      "name": "table_column_name",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
