from pyspark.sql.functions import concat_ws, collect_list
import json
from typing import Dict, Any
from openai.types.chat.chat_completion import ChatCompletion
from src.dbxmetagen.config import MetadataConfig
from src.dbxmetagen.error_handling import exponential_backoff
from src.dbxmetagen.chat_client import ChatClientFactory


class TableCommentSummarizer:
    def __init__(self, config, df):
        self.config = config
        self.df = df
        self.chat_client = ChatClientFactory.create_client(config)

    @staticmethod
    def _parse_table_comment_input(df):
        return df.toPandas().to_dict(orient="records")

    def _get_chat_completion(
        self,
        config: MetadataConfig,
        prompt_content: str,
        model: str,
        max_tokens: int,
        temperature: float,
        retries: int = 0,
        max_retries: int = 3,
    ) -> ChatCompletion:
        try:
            return self.chat_client.create_completion(
                messages=prompt_content,
                model=model,
                max_tokens=max_tokens,
                temperature=temperature,
            )
        except Exception as e:
            if retries < max_retries:
                print(f"Error: {e}. Retrying in {2 ** retries} seconds...")
                exponential_backoff(retries)
                return self._get_chat_completion(
                    config,
                    prompt_content,
                    model,
                    max_tokens,
                    temperature,
                    retries + 1,
                    max_retries,
                )
            else:
                print(f"Failed after {max_retries} retries.")
                raise e

    def _parse_response(self, response: str) -> Dict[str, Any]:
        try:
            response_dict = json.loads(response)
            if not isinstance(response_dict, dict):
                raise ValueError("Response is not a valid dict")
            return response_dict
        except json.JSONDecodeError as e:
            raise ValueError(f"JSON decode error: {e}")

    def summarize_comments(self, table_name: str) -> str:
        table_df = self.df.filter(self.df["table"] == table_name)

        # Limit to first 25 columns to keep prompt size reasonable
        column_count = table_df.count()
        limited_df = table_df.limit(25)

        comments = limited_df.select(
            concat_ws(" ", collect_list("column_content")).alias("all_comments")
        ).collect()[0]["all_comments"]

        # Add info about column limitation if applicable
        if column_count > 25:
            limitation_note = f" [Note: This table has {column_count} columns total - summarizing based on first 25 columns to maintain prompt efficiency.]"
            comments = comments + limitation_note
        prompt_content = [
            {
                "role": "system",
                "content": """You are an excellent AI assistant trying to help summarize multiple incomplete table summaries into one larger summary. You have significant experience at this and have been carefuly trained and tuned to make sure that you are as good as possible at providing high quality descriptions. All of the descriptions come from a single table, and were generated by a series of models. Please follow these instructions:

                ###
                Instructions:

                1. Please focus on the description of the table. Include descriptions of the types of columns, primary keys, foreign keys, and anything else you are provided with. Point out the relationships to other tables if the table has foreign keys.
                2. Please generate between 100 and 200 words, and 1 paragraph. But if you have significant information to include, not just guesses, add more information. The more columns the table has, the more meaningful metadata you are presented with, and if you are presented with significant information about the data in the tables, then spend more time elaborating, but if there is little to work with, then be more brief.
                3. Please make sure you use complete sentences.
                4. Do not include any violent, offensive, racist, or sexually explicit content.
                5. Focus on generating meaningful content that is useful to users of a database table, rather than guessing or generating filler content.
                6. Please review your response for hallucinations and duplication, and be very careful not to duplicate sentences or paragraphs, and be careful not to allow hallucinations to be provided as content.
                7. Please use language implying certainty and facts when you have facts, and implying uncertainty when you have uncertainty.
                8. Do not make any statements like "However, without more detailed information about the table's schema and content it is difficult to provide a more specific description of its structure and potential uses." You have all the information that you need to provide as meaningful a description of the table's structure and potential uses as possible.
                9. Try to only include information that actively adds value. Avoid repeating information, do not include relevant information, and avoid verbose language. 
                10. Be concise unless verbosity is truly warranted.
                11. Do not add headers or other markdown.
                12. I will provide you with a few-shot learning example below. Pattern your approach off this example, but do not use information from it directly unless the user input is nearly exactly the same.
                """,
            },
            {
                "role": "user",
                "content": """This table contains calculated values related to clinical trials, including facility counts and subject counts for adverse events. Based on the NCT IDs, this appears to be derived from ClinicalTrials.gov data, tracking the number of facilities and subjects involved in clinical trials. The table includes metrics on both non-serious adverse events (NSAE) and serious adverse events (SAE) subjects, providing insights into trial safety profiles. This table contains calculated values related to clinical trial registrations and reporting timelines. It tracks when trials were registered, their duration, whether results were reported, and the time taken to report results. The data appears to be from a clinical trials database, possibly tracking compliance with reporting requirements. This table contains calculated values related to facilities and age requirements, likely for clinical trials or healthcare services. The data includes boolean flags for facility locations and counts, as well as minimum and maximum age specifications with their corresponding units. Based on the high number of null values across columns, this appears to be a sparse dataset where not all records have complete information. Table containing calculated values related to clinical trial measurements and outcomes. This appears to be a dataset tracking different types of outcomes to measure in clinical trials along with age units. The table is part of the dbxmetagen.test_data schema, suggesting it may be test data for Databricks metadata generation. Based on the column metadata, this table contains a significant number of null values across all columns.""",
            },
            {
                "role": "user",
                "content": """This table in the dbxmetagen.test_data schema contains calculated metrics related to clinical trials, likely derived from ClinicalTrials.gov data. It tracks various aspects of clinical trials including facility information (counts and location flags), subject enrollment data, adverse event reporting (both serious adverse events (SAE) and non-serious adverse events (NSAE)), registration timelines, and reporting compliance metrics. The table includes age requirement specifications with corresponding units (minimum and maximum age values), as well as outcome measurement data. The presence of NCT IDs, the ClinicalTrials.gov identifier, suggests these are standardized clinical trial identifiers. The table is relatively sparse, with a significant number of null values across multiple columns, indicating that not all metrics are available for every trial. This comprehensive dataset serves analytical purposes for monitoring clinical trial safety profiles, regulatory compliance regarding result reporting timeframes, geographical distribution of trial facilities, and demographic inclusion criteria across different studies.""",
            },
            {"role": "user", "content": f"{comments}"},
        ]
        chat_completion = self._get_chat_completion(
            self.config,
            prompt_content,
            model=self.config.model,
            max_tokens=5000,
            temperature=0.3,
        )
        response_content = chat_completion.choices[0].message.content
        return response_content
