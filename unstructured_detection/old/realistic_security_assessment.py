# Databricks notebook source
# MAGIC %md
# MAGIC # Realistic Security Assessment: GLiNER-biomed
# MAGIC
# MAGIC This notebook provides a balanced, realistic security assessment focusing on:
# MAGIC 1. Actual security vulnerabilities vs theoretical risks
# MAGIC 2. Relevant threats for your use case
# MAGIC 3. Comparison to similar open source ML models
# MAGIC 4. Practical security mitigations

# COMMAND ----------

import subprocess
import sys
import requests
import json
from typing import Dict, Any, List
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# COMMAND ----------

# MAGIC %md
# MAGIC ## 1. Actual Vulnerability Assessment

# COMMAND ----------


def assess_gliner_vulnerabilities() -> Dict[str, Any]:
    """Get specific details about GLiNER package vulnerabilities."""

    vulnerability_report = {
        "package": "gliner==0.2.5",
        "assessment_date": "2024-12-19",
        "methodology": "Multiple security sources",
        "findings": [],
    }

    # Check if package is installed and get version info
    try:
        result = subprocess.run(
            [sys.executable, "-m", "pip", "show", "gliner"],
            capture_output=True,
            text=True,
            timeout=30,
        )

        if result.returncode == 0:
            vulnerability_report["installed_info"] = result.stdout
        else:
            vulnerability_report["installed_info"] = "Package not currently installed"

    except Exception as e:
        vulnerability_report["error"] = f"Could not check installation: {str(e)}"

    # Based on security research findings
    vulnerability_report["reported_issues"] = {
        "source": "ReversingLabs Spectra Assure Community",
        "severity_count": 3,
        "severity_level": "Severe",
        "exploitation_status": "Active exploitation reported",
        "details": "Specific vulnerability details not publicly disclosed",
        "cvss_score": "Unknown - not assigned CVE numbers yet",
        "affected_versions": "Including 0.2.5",
    }

    # Risk context for your use case
    vulnerability_report["risk_context"] = {
        "deployment_environment": "Databricks (isolated)",
        "data_exposure": "Local processing only",
        "network_access": "Controlled by cluster configuration",
        "privilege_level": "Standard user context",
        "attack_surface": "Limited to notebook execution environment",
    }

    return vulnerability_report


vuln_assessment = assess_gliner_vulnerabilities()

print("ðŸ” GLiNER Package Vulnerability Assessment:")
print(f"   Package: {vuln_assessment['package']}")

reported = vuln_assessment["reported_issues"]
print(f"\nâš ï¸ Reported Security Issues:")
print(f"   Source: {reported['source']}")
print(
    f"   Severity: {reported['severity_level']} ({reported['severity_count']} issues)"
)
print(f"   Status: {reported['exploitation_status']}")
print(f"   Details: {reported['details']}")

context = vuln_assessment["risk_context"]
print(f"\nðŸ›¡ï¸ Risk Context for Your Environment:")
for aspect, description in context.items():
    print(f"   {aspect}: {description}")

# COMMAND ----------

# MAGIC %md
# MAGIC ## 2. Comparison to Similar Open Source ML Models

# COMMAND ----------


def compare_to_similar_models() -> Dict[str, Any]:
    """Compare GLiNER-biomed to other open source NER models."""

    similar_models = {
        "GLiNER-biomed": {
            "author": "Ihor (individual)",
            "backing": "University of Geneva + Knowledgator Engineering",
            "downloads": "~5,000+ (moderate)",
            "last_updated": "2024",
            "known_vulnerabilities": "3 severe (in base package)",
            "training_data": "Synthetic biomedical data",
            "medical_performance": "High (specialized for medical)",
            "risk_assessment": "Medium-High",
        },
        "spaCy_scispaCy": {
            "author": "Explosion AI + Allen Institute",
            "backing": "Commercial + Academic institutions",
            "downloads": "1M+ (very high)",
            "last_updated": "Regular updates",
            "known_vulnerabilities": "None currently reported",
            "training_data": "Scientific literature (vetted)",
            "medical_performance": "Good (scientific focus)",
            "risk_assessment": "Low",
        },
        "Transformers_BioBERT": {
            "author": "HuggingFace + BioBERT team",
            "backing": "HuggingFace + Academic (Korea University)",
            "downloads": "100,000+ (high)",
            "last_updated": "Regular updates",
            "known_vulnerabilities": "None in core model",
            "training_data": "PubMed + PMC (public biomedical text)",
            "medical_performance": "High (trained on biomedical text)",
            "risk_assessment": "Low-Medium",
        },
        "Stanza_Biomedical": {
            "author": "Stanford NLP Group",
            "backing": "Stanford University",
            "downloads": "50,000+ (moderate-high)",
            "last_updated": "Regular updates",
            "known_vulnerabilities": "None currently reported",
            "training_data": "Biomedical corpora (academic)",
            "medical_performance": "Good (biomedical trained)",
            "risk_assessment": "Low",
        },
    }

    return similar_models


model_comparison = compare_to_similar_models()

print("ðŸ”¬ Comparison to Similar Open Source Medical NER Models:")
print("=" * 70)

for model_name, details in model_comparison.items():
    print(f"\n{model_name}:")
    for aspect, value in details.items():
        status_icon = (
            "âœ…"
            if "Low" in str(value)
            or "high" in str(value).lower()
            or "regular" in str(value).lower()
            else "âš ï¸" if "Medium" in str(value) else "âŒ"
        )
        print(
            f"   {aspect}: {value} {status_icon if aspect in ['risk_assessment', 'known_vulnerabilities'] else ''}"
        )

# COMMAND ----------

# MAGIC %md
# MAGIC ## 3. Relevant Threat Analysis

# COMMAND ----------


def analyze_relevant_threats() -> Dict[str, Any]:
    """Analyze threats that are actually relevant to your use case."""

    threat_analysis = {
        "RELEVANT_THREATS": {
            "code_execution_vulnerabilities": {
                "description": "Malicious code execution through model loading/inference",
                "likelihood": "Medium (known vulnerabilities exist)",
                "impact": "High (could access cluster resources)",
                "mitigation": "Network isolation, sandboxing, monitoring",
                "your_risk": "Medium - mitigated by Databricks isolation",
            },
            "dependency_chain_attacks": {
                "description": "Malicious packages in dependency chain",
                "likelihood": "Low (established packages)",
                "impact": "High (full environment access)",
                "mitigation": "Dependency pinning, security scanning",
                "your_risk": "Low - using pinned versions",
            },
            "model_backdoors": {
                "description": "Hidden functionality in model weights",
                "likelihood": "Very Low (no evidence, hard to implement)",
                "impact": "Medium (could affect predictions)",
                "mitigation": "Model validation, output monitoring",
                "your_risk": "Very Low - would be detectable",
            },
            "supply_chain_compromise": {
                "description": "Author account compromise, malicious model updates",
                "likelihood": "Low (requires targeted attack)",
                "impact": "High (if auto-updating)",
                "mitigation": "Version pinning, update review process",
                "your_risk": "Low - using pinned version",
            },
        },
        "IRRELEVANT_THREATS": {
            "training_data_exposure": {
                "description": "Model trained on sensitive data",
                "relevance": "Not relevant - wasn't trained on YOUR data",
                "your_risk": "None",
            },
            "compliance_violations": {
                "description": "Using model violates compliance",
                "relevance": "Not inherently relevant - depends on your compliance requirements",
                "your_risk": "Assess based on your specific requirements",
            },
            "intellectual_property": {
                "description": "Model violates IP rights",
                "relevance": "Not relevant for functionality/security",
                "your_risk": "Legal risk only, not operational",
            },
            "model_performance_degradation": {
                "description": "Model becomes less accurate over time",
                "relevance": "Operational issue, not security threat",
                "your_risk": "Monitor model performance",
            },
        },
    }

    return threat_analysis


threat_analysis = analyze_relevant_threats()

print("ðŸŽ¯ RELEVANT THREAT ANALYSIS")
print("=" * 50)

print("\nâš ï¸ Threats You Should Actually Care About:")
for threat, details in threat_analysis["RELEVANT_THREATS"].items():
    print(f"\n{threat.upper()}:")
    print(f"   What: {details['description']}")
    print(f"   Likelihood: {details['likelihood']}")
    print(f"   Impact: {details['impact']}")
    print(f"   Your Risk: {details['your_risk']}")
    print(f"   Mitigation: {details['mitigation']}")

print(f"\nâœ… Threats You Can Ignore:")
for threat, details in threat_analysis["IRRELEVANT_THREATS"].items():
    print(f"\n{threat.upper()}: {details['relevance']}")

# COMMAND ----------

# MAGIC %md
# MAGIC ## 4. Realistic Risk Assessment

# COMMAND ----------


def calculate_realistic_risk() -> Dict[str, Any]:
    """Calculate risk based on actual threats and your environment."""

    risk_factors = {
        "vulnerability_severity": {
            "score": 8,  # High - known severe vulnerabilities
            "weight": 0.4,  # High weight - this is the main concern
            "rationale": "3 severe vulnerabilities with active exploitation",
        },
        "attack_surface": {
            "score": 3,  # Low-Medium - limited to Databricks environment
            "weight": 0.2,
            "rationale": "Isolated Databricks environment, controlled network access",
        },
        "dependency_risk": {
            "score": 4,  # Medium - individual maintainer
            "weight": 0.15,
            "rationale": "Individual contributor vs enterprise, but established model",
        },
        "data_sensitivity": {
            "score": 2,  # Low - not trained on your data
            "weight": 0.1,
            "rationale": "Model not trained on your sensitive data",
        },
        "mitigation_effectiveness": {
            "score": 6,  # Medium-High - good mitigations available
            "weight": 0.15,
            "rationale": "Strong mitigations possible in Databricks",
        },
    }

    # Calculate weighted risk score
    total_weighted_score = 0
    for factor, details in risk_factors.items():
        weighted_score = details["score"] * details["weight"]
        total_weighted_score += weighted_score
        details["weighted_contribution"] = weighted_score

    # Risk levels
    if total_weighted_score >= 7:
        risk_level = "HIGH"
        recommendation = "Implement strong mitigations or consider alternatives"
    elif total_weighted_score >= 5:
        risk_level = "MEDIUM"
        recommendation = "Acceptable with proper security controls"
    else:
        risk_level = "LOW"
        recommendation = "Minimal additional precautions needed"

    return {
        "total_score": round(total_weighted_score, 2),
        "risk_level": risk_level,
        "recommendation": recommendation,
        "risk_factors": risk_factors,
        "primary_concern": "Known vulnerabilities in GLiNER package",
        "key_mitigation": "Network isolation and monitoring",
    }


realistic_risk = calculate_realistic_risk()

print("ðŸ“Š REALISTIC RISK ASSESSMENT")
print("=" * 50)

print(f"\nOverall Risk Score: {realistic_risk['total_score']}/10")
print(f"Risk Level: {realistic_risk['risk_level']}")
print(f"Recommendation: {realistic_risk['recommendation']}")

print(f"\nðŸ“‹ Risk Factor Breakdown:")
for factor, details in realistic_risk["risk_factors"].items():
    print(
        f"   {factor}: {details['score']}/10 (weight: {details['weight']:.0%}) = {details['weighted_contribution']:.2f}"
    )
    print(f"      â†’ {details['rationale']}")

print(f"\nðŸŽ¯ Key Findings:")
print(f"   Primary Concern: {realistic_risk['primary_concern']}")
print(f"   Key Mitigation: {realistic_risk['key_mitigation']}")

# COMMAND ----------

# MAGIC %md
# MAGIC ## 5. Practical Security Mitigations

# COMMAND ----------

security_mitigations = {
    "IMMEDIATE_MITIGATIONS": [
        {
            "action": "Network Isolation",
            "description": "Configure Databricks cluster with no internet access during inference",
            "effectiveness": "High",
            "implementation": "Cluster policy: disable internet access for worker nodes",
        },
        {
            "action": "Version Pinning",
            "description": "Pin exact GLiNER version (0.2.5) to prevent automatic updates",
            "effectiveness": "Medium",
            "implementation": "Already done in requirements_ner.txt",
        },
        {
            "action": "Process Monitoring",
            "description": "Monitor GLiNER processes for unusual network/file activity",
            "effectiveness": "Medium",
            "implementation": "Enable Databricks audit logging and monitoring",
        },
        {
            "action": "Input Sanitization",
            "description": "Validate and sanitize input text to prevent injection attacks",
            "effectiveness": "Medium",
            "implementation": "Add input validation in UDF preprocessing",
        },
    ],
    "ONGOING_SECURITY_PRACTICES": [
        {
            "action": "Security Updates Monitoring",
            "description": "Monitor for GLiNER package security updates and patches",
            "frequency": "Weekly",
            "implementation": "Subscribe to security advisories, check GitHub issues",
        },
        {
            "action": "Alternative Evaluation",
            "description": "Regularly assess alternative models for security/performance",
            "frequency": "Quarterly",
            "implementation": "Benchmark against spaCy, BioBERT, etc.",
        },
        {
            "action": "Vulnerability Scanning",
            "description": "Scan dependencies for new vulnerabilities",
            "frequency": "Monthly",
            "implementation": "Use pip-audit or safety tools",
        },
    ],
    "ACCEPTABLE_RISK_SCENARIO": [
        "âœ… GLiNER shows significant performance advantage (>0.1 F1 improvement)",
        "âœ… Strong network isolation implemented",
        "âœ… Comprehensive monitoring in place",
        "âœ… Regular security review process established",
        "âœ… Risk documented and approved by security team",
    ],
}

print("ðŸ›¡ï¸ PRACTICAL SECURITY MITIGATIONS")
print("=" * 50)

print("\nðŸš¨ Immediate Actions:")
for mitigation in security_mitigations["IMMEDIATE_MITIGATIONS"]:
    print(f"\n{mitigation['action']}:")
    print(f"   Description: {mitigation['description']}")
    print(f"   Effectiveness: {mitigation['effectiveness']}")
    print(f"   Implementation: {mitigation['implementation']}")

print(f"\nðŸ”„ Ongoing Practices:")
for practice in security_mitigations["ONGOING_SECURITY_PRACTICES"]:
    print(f"\n{practice['action']} ({practice['frequency']}):")
    print(f"   What: {practice['description']}")
    print(f"   How: {practice['implementation']}")

print(f"\nâœ… Use GLiNER if ALL of these conditions are met:")
for condition in security_mitigations["ACCEPTABLE_RISK_SCENARIO"]:
    print(f"   {condition}")

# COMMAND ----------

# MAGIC %md
# MAGIC ## 6. Final Balanced Recommendation

# COMMAND ----------

final_recommendation = f"""
ðŸŽ¯ BALANCED SECURITY RECOMMENDATION

SECURITY REALITY CHECK:
âœ… GLiNER package has real vulnerabilities (not theoretical)
âœ… Your Databricks environment provides good isolation
âœ… Performance on medical text likely significantly better than general models
âœ… Risk is manageable with proper controls

DECISION FRAMEWORK:

IF performance difference > 0.1 F1 points:
   â†’ GLiNER acceptable WITH strong security controls
   â†’ Implement network isolation, monitoring, version pinning
   â†’ Document risk acceptance and mitigation plan

IF performance difference < 0.05 F1 points:  
   â†’ Consider alternatives (spaCy+scispaCy, BioBERT)
   â†’ Security risk not justified for minimal performance gain

IF performance difference 0.05-0.1 F1 points:
   â†’ Business decision based on:
     - Criticality of NER accuracy for your use case
     - Security team risk tolerance
     - Resources available for ongoing security monitoring

RECOMMENDED ALTERNATIVES TO EVALUATE:
1. spaCy + scispaCy (good medical performance, better security)
2. BioBERT via HuggingFace (high medical performance, better security)  
3. Stanza biomedical models (academic backing, good performance)

BOTTOM LINE:
The security concerns are REAL but MANAGEABLE. The decision should be based on:
1. Actual performance difference (run the comparison notebook)
2. Your organization's risk tolerance
3. Resources available for security controls

This is a legitimate engineering tradeoff, not a clear-cut security issue.
"""

print(final_recommendation)

# COMMAND ----------
